{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0049ba7b-d554-430f-a8ef-e89ecd0d73f4",
   "metadata": {},
   "source": [
    "# Chatbot de Agricultura Sostenible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141c6c81-f673-48f0-b225-562c5120d05e",
   "metadata": {},
   "source": [
    "Se realizó como proyecto una software de agricultura sostenible que funciona con un chatbot. Se realizó en el idioma español con la biblioteca SpaCy para utilizar un lematizador para los tokens de una oración y en base a esto el modelo puede clasificar en un archivo intents.JSON las posibles respuestas al usuario. La idea del proyecto es una funcionalidad en servidor con FLASK en la que por un API se va a mandar al dominio que tendrá la interfaz del chatbot para resolver problemas con respecto a plagas, hay un problema en el tokenizador en el chatbot de servidor donde solo funciona si se llena la oración en el mismo formato de caracteres que en el archivo JSON. Por medio del archivo Jupyter se tiene una interfaz local en la que el chatbot tiene corregido ese problema y el tokenizador funciona sin inconvenientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a0b64-ba46-4c7f-891a-2328a3218084",
   "metadata": {},
   "source": [
    "El chatbot funciona consultando cuando la planta tiene una de las siguientes plagas: Mancha bacteriana, Sarampion negro, pudrición negra, tizón temprano, tizón tardío, quemadura de la hoja, óxido, costra, mancha, también hay opción de desconocido y sin plaga. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8608c88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 8999  | total loss: \u001b[1m\u001b[32m0.00053\u001b[0m\u001b[0m | time: 0.027s\n",
      "| Adam | epoch: 1000 | loss: 0.00053 - acc: 1.0000 -- iter: 64/65\n",
      "Training Step: 9000  | total loss: \u001b[1m\u001b[32m0.00050\u001b[0m\u001b[0m | time: 0.029s\n",
      "| Adam | epoch: 1000 | loss: 0.00050 - acc: 1.0000 -- iter: 65/65\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\cardo\\Documents\\Samsung\\Chatbot prototipo\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Lemmatized 'sarampion' to ['sarampion']\n",
      "Bag of words for 'sarampion': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Model prediction for 'sarampion': [[2.95645342e-10 0.00000000e+00 0.00000000e+00 1.13007505e-29\n",
      "  0.00000000e+00 8.88616592e-03 1.12237138e-15 0.00000000e+00\n",
      "  1.32241957e-02 9.77889597e-01 6.65444873e-08 6.87948602e-20]]\n",
      "Lemmatized 'manchas' to ['mancha']\n",
      "Bag of words for 'manchas': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Model prediction for 'manchas': [[0.0000000e+00 2.3906308e-04 9.9971789e-01 7.4532631e-30 4.2966414e-05\n",
      "  0.0000000e+00 0.0000000e+00 7.2701217e-14 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "import nltk\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# Cargar modelo de spaCy en español\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "with open('intents.json', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub('[^a-zA-ZáéíóúÁÉÍÓÚñÑüÜ]', ' ', sentence)\n",
    "    doc = nlp(sentence)\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    print(f\"Lemmatized '{sentence}' to {lemmas}\")\n",
    "    return lemmas\n",
    "\n",
    "if os.path.exists(\"data.pickle\"):\n",
    "    os.remove(\"data.pickle\")\n",
    "if os.path.exists(\"model.tflearn.index\"):\n",
    "    os.remove(\"model.tflearn.index\")\n",
    "if os.path.exists(\"model.tflearn.meta\"):\n",
    "    os.remove(\"model.tflearn.meta\")\n",
    "if os.path.exists(\"model.tflearn.data-00000-of-00001\"):\n",
    "    os.remove(\"model.tflearn.data-00000-of-00001\")\n",
    "\n",
    "print(\"Archivos antiguos eliminados, procesando datos y entrenando el modelo desde cero\")\n",
    "\n",
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        wrds = nltk.word_tokenize(pattern)\n",
    "        docs_x.append(wrds)\n",
    "        docs_y.append(intent['tag'])\n",
    "        \n",
    "        words.extend(lemmatize_sentence(pattern))\n",
    "\n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n",
    "\n",
    "words = sorted(list(set(words)))\n",
    "labels = sorted(labels)\n",
    "\n",
    "training = []\n",
    "output = []\n",
    "\n",
    "out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "for x, doc in enumerate(docs_x):\n",
    "    bag = []\n",
    "    wrds = lemmatize_sentence(' '.join(doc))\n",
    "\n",
    "    for w in words:\n",
    "        if w in wrds:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "    training.append(bag)\n",
    "    output.append(output_row)\n",
    "\n",
    "training = np.array(training)\n",
    "output = np.array(output)\n",
    "\n",
    "with open(\"data.pickle\", \"wb\") as f:\n",
    "    pickle.dump((words, labels, training, output), f)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model.save(\"model.tflearn\")\n",
    "\n",
    "def bag_of_words(s, words):\n",
    "    s = s.lower()\n",
    "    s = re.sub('[^a-zA-ZáéíóúÁÉÍÓÚñÑüÜ]', ' ', s)\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "    s_words = lemmatize_sentence(s)\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "\n",
    "    print(f\"Bag of words for '{s}': {bag}\")\n",
    "    return np.array(bag)\n",
    "\n",
    "def chatbot_response(msg):\n",
    "    bow = bag_of_words(msg, words)\n",
    "    results = model.predict([bow])\n",
    "    print(f\"Model prediction for '{msg}': {results}\")\n",
    "    results_index = np.argmax(results)\n",
    "    tag = labels[results_index]\n",
    "\n",
    "    for tg in data['intents']:\n",
    "        if tg['tag'] == tag:\n",
    "            responses = tg['responses']\n",
    "            return random.choice(responses)\n",
    "\n",
    "    return \"Lo siento, no entiendo lo que quieres decir.\"\n",
    "\n",
    "base = Tk()  \n",
    "base.title(\"Chatbot\")  \n",
    "base.geometry(\"400x500\")  \n",
    "\n",
    "ChatLog = Text(base, bd=0, bg=\"white\", height=\"8\", width=\"50\", font=(\"Arial\", 12), wrap=WORD)\n",
    "ChatLog.config(foreground=\"black\")\n",
    "ChatLog.insert(END, \"SALUDOS BIENVENIDO\\n\\n\") \n",
    "ChatLog.place(x=6, y=6, height=386, width=370) \n",
    "\n",
    "scrollbar = Scrollbar(base, command=ChatLog.yview, cursor=\"heart\")\n",
    "ChatLog['yscrollcommand'] = scrollbar.set\n",
    "scrollbar.place(x=376, y=6, height=386)\n",
    "\n",
    "ChatLog.config(state=DISABLED)\n",
    "\n",
    "EntryBox = Text(base, bd=0, bg=\"white\", width=\"29\", height=\"5\", font=(\"Arial\", 12), wrap=WORD)\n",
    "EntryBox.place(x=6, y=401, height=90, width=265)\n",
    "\n",
    "def send(event=None):\n",
    "    msg = EntryBox.get(\"1.0\", 'end-1c').strip()\n",
    "    EntryBox.delete(\"0.0\", END)\n",
    "\n",
    "    if msg != '':\n",
    "        ChatLog.config(state=NORMAL)\n",
    "        ChatLog.insert(END, \"You: \" + msg + '\\n\\n')\n",
    "        ChatLog.config(foreground=\"black\")\n",
    "\n",
    "        res = chatbot_response(msg)\n",
    "        ChatLog.insert(END, \"ChatBOT: \" + res + '\\n\\n')\n",
    "        ChatLog.config(state=DISABLED)\n",
    "        ChatLog.yview(END)\n",
    "\n",
    "SendButton = Button(base, font=(\"Verdana\", 12, 'bold'), text=\"Send\", width=\"9\",\n",
    "                   height=5, bd=0, bg=\"blue\", activebackground=\"gold\",\n",
    "                   fg='#ffffff', command=send)\n",
    "SendButton.place(x=282, y=401, height=90)\n",
    "\n",
    "base.bind('<Return>', send)\n",
    "\n",
    "base.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f066f7-f6e0-45da-bdb0-de5ea30c6306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d74bae-c923-41eb-8255-b13a19a9a1f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
